# ðŸŽ¥ Video DeepFake Detection and Attribution (Video-DFDA)

> **A deep learning system for detecting and classifying AI-generated fake videos (Deepfakes) using XceptionNet architecture on the FaceForensics++ C23 dataset.**

---

## ðŸ§  Algorithm Overview

The **Video DFDA** pipeline leverages a **frame-based CNN classification** approach to detect fake videos.  
By extracting frames from videos and analysing them individually through a deep neural network (XceptionNet), the system identifies synthetic manipulations such as **FaceSwap**, **Face2Face**, and **NeuralTextures**.

### Key Steps:
1. **Frame Extraction** â€“ Sample frames from video files at equal intervals.  
2. **Feature Learning** â€“ Feed extracted frames into a CNN (Xception) pre-trained on ImageNet.  
3. **Prediction Aggregation** â€“ Average frame-wise predictions to determine the overall video authenticity.  
4. **Attribution** â€“ Classify the specific deepfake generation technique.

---

## ðŸ” Flow Chart

```
    A[Input Video File] --> B[Extract Frames (Every Nth Frame)]
    B --> C[Frame Preprocessing (Resize, Normalize)]
    C --> D[XceptionNet Feature Extraction]
    D --> E[Softmax Classification]
    E --> F[Aggregate Frame Probabilities]
    F --> G[Final Prediction (Real / Fake Type)]
    G --> H[Streamlit App Visualization]
````

---
<img width="1994" height="3289" alt="localhost_8501_ (3)" src="https://github.com/user-attachments/assets/2e55ec13-a8ba-45d2-8c32-a4e7f89b8451" />

## ðŸ“‚ Folder Structure

```
Video-DFDA/
â”‚
â”œâ”€â”€ app.py                         # Streamlit-based video inference app
â”œâ”€â”€ train.py                       # Model training and evaluation script
â”œâ”€â”€ dataset_index.csv              # Pre-generated frame index file
â”œâ”€â”€ VideoOut.txt                   # Training logs
â”œâ”€â”€ VIDEO_DFDA/
â”‚   â””â”€â”€ Results/
â”‚       â”œâ”€â”€ best_model.pt
â”‚       â”œâ”€â”€ metrics_log.csv
â”‚       â”œâ”€â”€ confusion_matrix.png
â”‚       â”œâ”€â”€ accuracy_curve.png
â”‚       â”œâ”€â”€ loss_curve.png
â”‚       â”œâ”€â”€ classification_report.txt
â”‚       â”œâ”€â”€ prec_recall_f1.png
â”‚       â””â”€â”€ roc_curves.png
â”‚
â”œâ”€â”€ Dataset/
â”‚   â””â”€â”€ FF++C32-Frames/            # Dataset (from Kaggle)
â”‚       â”œâ”€â”€ Deepfakes/
â”‚       â”œâ”€â”€ Face2Face/
â”‚       â”œâ”€â”€ FaceShifter/
â”‚       â”œâ”€â”€ FaceSwap/
â”‚       â”œâ”€â”€ NeuralTextures/
â”‚       â””â”€â”€ Original/
â”‚
â””â”€â”€ README.md
```

---

## âœ¨ Features

* ðŸŽž **Frame-based DeepFake Detection**
* ðŸ§  **XceptionNet architecture (from TIMM)**
* ðŸ“Š **Per-class precision, recall, and F1 visualization**
* ðŸ“ˆ **ROC curves, confusion matrices, and accuracy plots**
* ðŸŒ **Streamlit interface for interactive analysis**
* âš™ï¸ **Automatic model checkpointing and training reports**

---

## ðŸ“˜ Dataset

**Dataset Used:** [FaceForensics++ Extracted Dataset (C23 Quality)](https://www.kaggle.com/datasets/fatimahirshad/faceforensics-extracted-dataset-c23)

### Dataset Description

| Attribute    | Details                                                                               |
| :----------- | :------------------------------------------------------------------------------------ |
| Source       | FaceForensics++ C23 subset                                                            |
| Type         | Frame-level deepfake images                                                           |
| Classes      | 6 (`Deepfakes`, `Face2Face`, `FaceShifter`, `FaceSwap`, `NeuralTextures`, `Original`) |
| Format       | JPG/PNG                                                                               |
| Total Frames | ~20,000+                                                                              |
| Resolution   | 256Ã—256 â€“ 512Ã—512                                                                     |

---

## ðŸ§© Adding the Dataset

1. **Install Kaggle CLI**

   ```bash
   pip install kaggle
   ```

2. **Authenticate**

   * Download `kaggle.json` from Kaggle â†’ Account â†’ API â†’ Create New Token
   * Place in:

     ```
     ~/.kaggle/kaggle.json
     ```
   * Then run:

     ```bash
     chmod 600 ~/.kaggle/kaggle.json
     ```

3. **Download the Dataset**

   ```bash
   kaggle datasets download -d fatimahirshad/faceforensics-extracted-dataset-c23 -p ./Dataset --unzip
   ```

4. **Expected Folder Layout**

   ```
   Dataset/
   â””â”€â”€ FF++C32-Frames/
       â”œâ”€â”€ Deepfakes/
       â”œâ”€â”€ Face2Face/
       â”œâ”€â”€ FaceShifter/
       â”œâ”€â”€ FaceSwap/
       â”œâ”€â”€ NeuralTextures/
       â””â”€â”€ Original/
   ```

---

## âš™ï¸ Installing Requirements

```bash
pip install torch torchvision timm opencv-python numpy pandas matplotlib seaborn scikit-learn streamlit
```

Or install from `requirements.txt` (if provided).

---

## ðŸ‹ï¸ Training the Model

Run the training script:

```bash
python train.py
```

### Training Pipeline

1. **Create Frame Index CSV** (`dataset_index.csv`)
   Automatically scans `Dataset/FF++C32-Frames` and assigns labels.
   *(Auto-created on first run.)*

2. **Train/Test Split**

   * 80% training, 20% validation.

3. **Model Architecture**

   * **XceptionNet** (from `timm`)
   * Input Size: 300Ã—300
   * Batch Size: 16
   * Optimizer: Adam
   * Scheduler: ReduceLROnPlateau

4. **Metrics Tracked**

   * Training/Validation Loss
   * Accuracy
   * Confusion Matrix
   * ROC Curves
   * Precision/Recall/F1 per class

---

## ðŸ“Š Training Metrics (From `VideoOut.txt`)

| Metric        |          Training | Validation |
| :------------ | ----------------: | ---------: |
| Best Accuracy |             99.6% | **90.00%** |
| Loss (final)  |             0.011 |     0.4228 |
| Epochs        |                15 |            |
| Model         | `legacy_xception` |            |
| Time          |           219 min |            |

---

## ðŸ§  Model Architecture

```
Input: 300x300 RGB Frame
â”‚
â”œâ”€â”€ XceptionNet (Pretrained on ImageNet)
â”‚   â”œâ”€â”€ Depthwise Separable Convolutions
â”‚   â”œâ”€â”€ Residual Connections
â”‚   â””â”€â”€ Global Average Pooling
â”‚
â””â”€â”€ Fully Connected Layer (6 Classes)
    â†’ Softmax Output
```

**Classes:**

* Deepfakes
* Face2Face
* FaceShifter
* FaceSwap
* NeuralTextures
* Original

---

## ðŸ§¾ Results

| Class          | Precision | Recall |   F1 |
| :------------- | --------: | -----: | ---: |
| Deepfakes      |      0.89 |   0.88 | 0.88 |
| Face2Face      |      0.90 |   0.87 | 0.88 |
| FaceShifter    |      0.92 |   0.91 | 0.91 |
| FaceSwap       |      0.88 |   0.89 | 0.88 |
| NeuralTextures |      0.87 |   0.86 | 0.86 |
| Original       |      0.99 |   1.00 | 0.99 |

**Validation Accuracy:** 90.0%
**Weighted F1:** 0.89

---

## ðŸš€ Using Streamlit for Video DFDA

Launch the video analysis dashboard:

```bash
streamlit run app.py
```

### Features:

* ðŸŽž Upload any `.mp4`, `.avi`, or `.mkv` file.
* âš™ï¸ Automatically extracts frames every 10th frame.
* ðŸ“Š Displays class probabilities as a bar chart and pie chart.
* ðŸŽ¯ Shows final predicted label with confidence score.
* ðŸ’¾ Downloadable analysis report.

Example output:

```
ðŸŽ¯ Predicted Class: Deepfakes (91.8% confidence)
```

Local Access: [http://localhost:8501](http://localhost:8501)

---

## ðŸ§¾ Project Structure (Detailed)

```
Video-DFDA/
â”‚
â”œâ”€â”€ train.py                   # Training and evaluation logic
â”œâ”€â”€ app.py                     # Streamlit interface
â”œâ”€â”€ dataset_index.csv          # Frame metadata
â”œâ”€â”€ VideoOut.txt               # Training output
â”œâ”€â”€ VIDEO_DFDA/
â”‚   â””â”€â”€ Results/
â”‚       â”œâ”€â”€ best_model.pt
â”‚       â”œâ”€â”€ loss_curve.png
â”‚       â”œâ”€â”€ accuracy_curve.png
â”‚       â”œâ”€â”€ confusion_matrix.png
â”‚       â”œâ”€â”€ classification_report.txt
â”‚       â”œâ”€â”€ prec_recall_f1.png
â”‚       â”œâ”€â”€ roc_curves.png
â”‚       â””â”€â”€ run_metadata.csv
â”‚
â”œâ”€â”€ Dataset/
â”‚   â””â”€â”€ FF++C32-Frames/
â”‚       â”œâ”€â”€ Deepfakes/
â”‚       â”œâ”€â”€ Face2Face/
â”‚       â”œâ”€â”€ FaceShifter/
â”‚       â”œâ”€â”€ FaceSwap/
â”‚       â”œâ”€â”€ NeuralTextures/
â”‚       â””â”€â”€ Original/
```

---

## âš™ï¸ Configuration

| Parameter     |                            Default | Description             |
| :------------ | ---------------------------------: | :---------------------- |
| `IMAGE_SIZE`  |                                300 | Frame resize dimension  |
| `BATCH_SIZE`  |                                 16 | Batch size for training |
| `NUM_EPOCHS`  |                                 15 | Total epochs            |
| `LR`          |                               1e-4 | Learning rate           |
| `NUM_WORKERS` |                                  4 | Dataloader threads      |
| `MODEL_PATH`  | `VIDEO_DFDA/Results/best_model.pt` | Streamlit model path    |

---

## ðŸ§° Troubleshooting

| Issue                                  | Cause                | Fix                                  |
| :------------------------------------- | :------------------- | :----------------------------------- |
| `FileNotFoundError: dataset_index.csv` | Missing CSV          | Run `train.py` once to auto-generate |
| CUDA memory error                      | Low GPU memory       | Reduce `BATCH_SIZE`                  |
| Streamlit model mismatch               | Incorrect model path | Update `MODEL_PATH` in `app.py`      |
| Slow frame extraction                  | Large video          | Increase `frame_skip` parameter      |

---

## ðŸ¤ Contributing

1. Fork the repository
2. Create a branch: `git checkout -b feature/video-updates`
3. Commit changes: `git commit -m "Improve video frame handling"`
4. Push: `git push origin feature/video-updates`
5. Open a Pull Request

---

## ðŸ“œ License

Released under the **MIT License**.
Free for research and educational use with proper attribution.

---

## ðŸ™ Acknowledgements

* **Dataset:** [FaceForensics++ Extracted Dataset C23](https://www.kaggle.com/datasets/fatimahirshad/faceforensics-extracted-dataset-c23)
* **Frameworks:** PyTorch, Streamlit, OpenCV, TIMM, Scikit-Learn
* **Hardware:** NVIDIA RTX 3050 6GB GPU
* **Author:** [Jayanth Bottu](https://www.linkedin.com/in/jayanthbottu/)

---

## ðŸ“ž Contact

**Author:** Jayanth Bottu
ðŸ”— LinkedIn: [linkedin.com/in/jayanthbottu](https://www.linkedin.com/in/jayanthbottu/)

---

## âš ï¸ Note

> **This is a research and educational project.**
> For real-world or production deployment, additional robustness testing, bias evaluation, and multi-environment validation are required.
