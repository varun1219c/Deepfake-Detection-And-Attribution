# ðŸŽ§ Audio DeepFake Detection and Attribution (Audio-DFDA)

> **AI-based detection and attribution of synthetic voice content generated using speech synthesis or deepfake models.**

---

## ðŸ§  Algorithm Overview

The **Audio DFDA** system employs a **Convolutional Neural Network (CNN)** classifier trained on **Mel-spectrogram representations** of audio signals to detect and attribute audio deepfakes.

### Core Pipeline
1. **Audio Preprocessing**
   - Resample all audio clips to 44.1 kHz.
   - Normalize amplitude and duration.
   - Convert waveform â†’ Mel-Spectrogram.
2. **Feature Extraction**
   - Compute 128 Mel bands.
   - Apply dB scaling for visual consistency.
3. **CNN Classification**
   - CNN processes the 2D Mel-Spectrograms.
   - Extracts frequencyâ€“time features distinctive of synthetic speech.
4. **Attribution**
   - Classifies audio as **Real** or generated by one of several **AI Voice Synthesis Models**.

---

## ðŸ” Flow Chart
```
    A[Input Audio File (.wav/.mp3)] --> B[Resampling and Normalization]
    B --> C[Mel-Spectrogram Conversion]
    C --> D[Deep CNN Model Training]
    D --> E[Model Evaluation (Accuracy, F1, Loss)]
    E --> F[Save Best Model Checkpoint]
    F --> G[Streamlit Web Interface]
    G --> H[User Uploads Audio for Prediction]
    H --> I[Confidence Visualization and Report Download]
```

---
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/1d2a68a7-f809-4360-8786-c0fb80b5856e" />

## ðŸ“‚ Folder Structure

```
Audio-DFDA/
â”‚
â”œâ”€â”€ app.py                         # Streamlit web interface
â”œâ”€â”€ training.py                    # Model training + evaluation
â”œâ”€â”€ requirements.txt                # Dependency list
â”œâ”€â”€ AudioOut.txt                    # Training logs and metrics
â”‚
â”œâ”€â”€ saved_models/                   # Trained model checkpoints
â”‚   â””â”€â”€ best_model_*.pt
â”‚
â”œâ”€â”€ training_reports/               # Plots & metrics
â”‚   â””â”€â”€ YYYYMMDD_HHMMSS/
â”‚       â”œâ”€â”€ metrics/
â”‚       â””â”€â”€ plots/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ audio_dfda/                 # Downloaded dataset folders
â”‚       â”œâ”€â”€ ElevenLabs/
â”‚       â”œâ”€â”€ Tacotron/
â”‚       â”œâ”€â”€ TextToSpeech/
â”‚       â”œâ”€â”€ VoiceConversion/
â”‚       â””â”€â”€ Original/
â”‚
â””â”€â”€ README.md
```

---

## âœ¨ Features

* ðŸŽµ **Automatic DeepFake Audio Detection**
* ðŸ§  **Attribution across multiple generation techniques**
* ðŸ“Š **Comprehensive training analytics (F1, Precision, Recall)**
* ðŸ–¥ï¸ **Streamlit dashboard for real-time inference**
* âš™ï¸ **GPU-optimized training pipeline**
* ðŸ“ˆ **Interactive visualizations of results**

---

## ðŸ“˜ Dataset

**Dataset Used:** [Audio DeepFake Dataset by Jayanth Bottu](https://www.kaggle.com/datasets/jayanthbottu/audio-deepfake)

### Details:

| Attribute     | Description                                                       |
| :------------ | :---------------------------------------------------------------- |
| Total Samples | ~2,561 audio files                                                |
| Sample Rate   | 44.1 kHz                                                          |
| Classes       | 5 (ElevenLabs, Tacotron, TextToSpeech, VoiceConversion, Original) |
| Format        | WAV / MP3                                                         |
| Duration      | 3â€“10 seconds per clip                                             |

---

## ðŸ§© Adding the Dataset

1. **Install Kaggle CLI**

   ```bash
   pip install kaggle
   ```

2. **Setup Authentication**

   * Go to your Kaggle account â†’ Create New API Token.
   * Save the `kaggle.json` file in:

     ```
     ~/.kaggle/kaggle.json
     ```
   * Run:

     ```bash
     chmod 600 ~/.kaggle/kaggle.json
     ```

3. **Download Dataset**

   ```bash
   kaggle datasets download -d jayanthbottu/audio-deepfake -p ./data/audio_dfda --unzip
   ```

4. **Verify Structure**

   ```
   data/audio_dfda/
   â”œâ”€â”€ ElevenLabs/
   â”œâ”€â”€ Tacotron/
   â”œâ”€â”€ TextToSpeech/
   â”œâ”€â”€ VoiceConversion/
   â””â”€â”€ Original/
   ```

---

## âš™ï¸ Installing Requirements

Ensure Python â‰¥ 3.10 and CUDA 11.8 (for GPU users).

```bash
pip install -r requirements.txt
```

### Key Libraries:

* **torch**, **torchaudio**
* **librosa**
* **scikit-learn**
* **matplotlib**, **seaborn**, **plotly**
* **streamlit**

---

## ðŸ‹ï¸ Training the Model

Run training using:

```bash
python training.py
```

### Training Steps

1. Data split: 70% train / 15% validation / 15% test
2. Feature extraction: Mel-Spectrogram generation
3. Model: 4-layer CNN with batch normalization and dropout
4. Loss: CrossEntropy with label smoothing
5. Optimizer: AdamW + ReduceLROnPlateau scheduler
6. Early stopping: Stops if validation F1 stagnates for 5 epochs

---

## ðŸ“Š Training Metrics

| Metric      |              Train |        Validation |
| :---------- | -----------------: | ----------------: |
| Accuracy    |              99.3% | **98.25% (Best)** |
| Weighted F1 |              0.992 |             0.981 |
| Epochs      | 24 (early stopped) |                   |

### Output Files

* **Plots:** ROC, Confusion Matrix, Precision-Recall
* **Reports:** `training_summary.json`, `classification_report.csv`
* **Best Model:** `saved_models/best_model_xxxxx.pt`

---

## ðŸ§  Model Architecture

```
Input: Mel-Spectrogram (128x128)
â”‚
â”œâ”€â”€ Conv2D(32, 3x3) + BN + ReLU + MaxPool
â”œâ”€â”€ Conv2D(64, 3x3) + BN + ReLU + MaxPool
â”œâ”€â”€ Conv2D(128, 3x3) + BN + ReLU + Dropout(0.3)
â”œâ”€â”€ Flatten
â”œâ”€â”€ Dense(256) + ReLU + Dropout(0.5)
â””â”€â”€ Dense(5) + Softmax
```

**Total Parameters:** ~4.85 million

---

## ðŸ§¾ Results

| Class           | Precision | Recall |   F1 |
| :-------------- | --------: | -----: | ---: |
| ElevenLabs      |      0.98 |   0.99 | 0.98 |
| Tacotron        |      0.99 |   0.98 | 0.98 |
| TextToSpeech    |      0.97 |   0.97 | 0.97 |
| VoiceConversion |      0.98 |   0.99 | 0.98 |
| Original        |      1.00 |   1.00 | 1.00 |

**Overall Accuracy:** 99.37%
**Weighted F1:** 0.9937

---

## ðŸŒ Using Streamlit for Audio DFDA

Run:

```bash
streamlit run app.py
```

### Features:

* Upload `.wav` / `.mp3` audio.
* View:

  * Waveform
  * Mel-Spectrogram
  * Predicted Class + Confidence
* Download report
* Real-time probability bars

Local link: [http://localhost:8501](http://localhost:8501)

---

## ðŸ§¾ Project Structure (Detailed)

```
Audio-DFDA/
â”‚
â”œâ”€â”€ data/audio_dfda/                 # Dataset
â”‚   â”œâ”€â”€ ElevenLabs/
â”‚   â”œâ”€â”€ Tacotron/
â”‚   â”œâ”€â”€ TextToSpeech/
â”‚   â”œâ”€â”€ VoiceConversion/
â”‚   â””â”€â”€ Original/
â”‚
â”œâ”€â”€ saved_models/                    # Saved models
â”‚   â””â”€â”€ best_model_20251008_194321.pt
â”‚
â”œâ”€â”€ training_reports/                # Metrics and plots
â”‚   â””â”€â”€ 20251008_194321/
â”‚       â”œâ”€â”€ metrics/
â”‚       â”‚   â”œâ”€â”€ classification_report.csv
â”‚       â”‚   â”œâ”€â”€ detailed_metrics.json
â”‚       â”‚   â””â”€â”€ training_summary.json
â”‚       â””â”€â”€ plots/
â”‚           â”œâ”€â”€ confusion_matrices.png
â”‚           â”œâ”€â”€ roc_curve.png
â”‚           â”œâ”€â”€ precision_recall_curve.png
â”‚           â””â”€â”€ performance_dashboard.png
â”‚
â”œâ”€â”€ app.py                           # Streamlit app
â”œâ”€â”€ training.py                      # Training pipeline
â”œâ”€â”€ requirements.txt                 # Dependencies
â””â”€â”€ AudioOut.txt                     # Training log
```

---

## âš™ï¸ Configuration

| Parameter                 | Default | Description         |
| :------------------------ | ------: | :------------------ |
| `NUM_EPOCHS`              |      50 | Max training epochs |
| `LEARNING_RATE`           |  0.0001 | Initial LR          |
| `BATCH_SIZE`              |      16 | Batch size          |
| `DROPOUT_RATE`            |     0.5 | Regularisation      |
| `EARLY_STOPPING_PATIENCE` |       5 | Stop criteria       |
| `LR_REDUCTION_FACTOR`     |     0.5 | Learning rate decay |

All configurable in `training.py`.

---

## ðŸ§° Troubleshooting

| Issue             | Cause             | Fix                                           |
| :---------------- | :---------------- | :-------------------------------------------- |
| Dataset not found | Incorrect path    | Place dataset under `data/audio_dfda/`        |
| CUDA error        | Insufficient VRAM | Reduce batch size to 4                        |
| `ffmpeg` error    | Missing library   | Install: `sudo apt install ffmpeg`            |
| Streamlit error   | Port conflict     | Run `streamlit run app.py --server.port 8502` |

---

## ðŸ¤ Contributing

Contributions are welcome!

1. **Fork** the repository
2. **Create a branch:** `git checkout -b feature/new-feature`
3. **Commit changes:** `git commit -m "Add new feature"`
4. **Push:** `git push origin feature/new-feature`
5. **Open Pull Request**

---

## ðŸ“œ License

Licensed under the **MIT License**.
You may use, modify, and distribute this project with attribution.

---

## ðŸ™ Acknowledgements

* Dataset: [Audio DeepFake Dataset by Jayanth Bottu](https://www.kaggle.com/datasets/jayanthbottu/audio-deepfake)
* Libraries: PyTorch, Torchaudio, Librosa, Streamlit, Plotly, Scikit-Learn
* Hardware: NVIDIA RTX 3050 6GB GPU

---

## ðŸ“ž Contact

**Author:** Jayanth Bottu
ðŸ”— LinkedIn: [linkedin.com/in/jayanthbottu](https://www.linkedin.com/in/jayanthbottu/)

---

## âš ï¸ Note

> **This is a research/educational project.**
> For production use, additional validation, adversarial testing, and real-world data evaluation are strongly recommended.
